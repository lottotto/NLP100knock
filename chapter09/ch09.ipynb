{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法(I)\n",
    "enwiki-20150112-400-r10-105752.txt.bz2は、2015年1月12日時点の英語のwikipedia記事のうち、約400語以上で構成される記事の中からランダムにサンプリングした105752記事のテキストをbzip2形式で圧縮したものである。このテキストをコーパスとして単語の意味を表すベクトル(分散表現)を学習したい。第9章の前半では、コーパスから作成した単語文脈共起行列に主成分分析を適用し、単語ベクトルを学習する過程を、いくつかの処理に分けて実装する。第9章の後半では、学習で得られた単語ベクトル(300語)を用い、単語の類似度計算やアナロジー(類推)を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. コーパスの整形\n",
    "文を単語列に変換する最も単純な方法は、空白文字で単語を区切ることである。ただこの方法では文末のピリオドや括弧などの記号が単語に含まれてしまう。そこでコーパスの各行のテキストを空白文字でトークンのリストに分割したのち、各トークンに以下の処理を施し、単語から記号を除去せよ。\n",
    "\n",
    "* トークンの先頭と末尾に出現する次の文字を削除: `.,!?;:()[]'\"`\n",
    "* 空文字となったトークンは削除  \n",
    "以上の処理を適用した後、トークンをスペースで連結してファイルに保存せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_punctuation = lambda x:re.sub('[.,!?;:()\\[\\]\\'\"]','',x)\n",
    "Sentence = open('./enwiki-20150112-400-r10-105752.txt','r')\n",
    "ret_file = open('./formated_corpus.txt', 'w')\n",
    "for line in Sentence:\n",
    "    temp_line = line.split(' ')\n",
    "    temp_line = list(map(rm_punctuation, temp_line))\n",
    "    temp_line = list(filter(lambda x:x!='', temp_line))\n",
    "    temp_line = \" \".join(temp_line)\n",
    "    ret_file.writelines(temp_line)\n",
    "ret_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81.複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = open('./Country.txt','r')\n",
    "#国複合語を抽出\n",
    "complex_words = {}\n",
    "for country in countries:\n",
    "    if len(country.split()) > 1:\n",
    "        complex_words[country.split()[0]] = country.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./formated_corpus.txt','r')\n",
    "ret_file = open('./complex_words_conbine.txt','w')\n",
    "\n",
    "skip_count = 0\n",
    "for line in file:\n",
    "    temp_line = []\n",
    "    for i, token in enumerate(line.split(' ')):\n",
    "        \n",
    "        if skip_count > 0:\n",
    "            skip_count -=1\n",
    "            continue\n",
    "        \n",
    "        if token in complex_words:\n",
    "            complex_word_List = complex_words[token].split()\n",
    "            flag = True\n",
    "            for j in range(len(complex_word_List)):\n",
    "                essense = complex_word_List[j]\n",
    "                try:\n",
    "                    temp = line.split(' ')[i+j]\n",
    "                except:\n",
    "                    temp = \"XXX\"\n",
    "                if temp != essense:\n",
    "                    flag = False\n",
    "                    \n",
    "            if flag:\n",
    "                temp_line.append(\"_\".join(complex_word_List))\n",
    "                skip_count = len(complex_word_List)\n",
    "        else:\n",
    "            temp_line.append(token)\n",
    "    ret_file.write(\" \".join(temp_line))\n",
    "ret_file.close()\n",
    "                           \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語*t*に関して単語*t*と文脈語*c*のペアをタブ区切りですべて書き出せ。ただし文脈語の定義は以下の通り。\n",
    "* ある単語*t*の前後*d*単語を文脈語cとして出力する(ただし、文脈語に単語*t*そのものは含まない).\n",
    "* 単語*t* を選ぶ度に、文脈幅*d*は```{1,2,3,4,5}```の範囲でランダムに決める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "short_src = open('./short.txt','r')\n",
    "file = open('./complex_words_conbine.txt','r')\n",
    "ret_file = open('./token_context.txt','w')\n",
    "for line in file:\n",
    "    iter_tokens  = line.split()\n",
    "    if len(iter_tokens) == 1:\n",
    "        continue\n",
    "    \n",
    "    for i in range(1,len(iter_tokens)):\n",
    "        if i < 5:\n",
    "            d = randint(1,i)\n",
    "        elif len(iter_tokens)-i<5:\n",
    "            d = randint(1,len(iter_tokens)-i)\n",
    "        else:\n",
    "            d = randint(1,5)\n",
    "        ret_file.write(iter_tokens[i]+\"\\t\"+\"\\t\".join(iter_tokens[i-d:i]+iter_tokens[i+1:i+d+1])+'\\n')\n",
    "    \n",
    "ret_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. 単語/ 文脈の頻度の計測\n",
    "82の出力を利用し、以下の出現分布、および定数を求めよ\n",
    "* $f(t,c)$: 単語*t*と文脈語*c*の共起回数\n",
    "* $f(t,*)$: 単語*t*の出現回数\n",
    "* $f(*,c)$: 文脈語cの出現回数\n",
    "* $N$:単語と文脈語のペアの総出現回数\n",
    "\n",
    "```bash:\n",
    "split -l 10000000 ./token_context.txt ./token_context_split\n",
    "```\n",
    "を実行しファイルを分割しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000000it [01:15, 131918.79it/s]\n",
      "10000000it [01:14, 133976.65it/s]\n",
      "10000000it [01:14, 134956.80it/s]\n",
      "10000000it [01:14, 135128.10it/s]\n",
      "10000000it [01:15, 131606.03it/s]\n",
      "10000000it [01:14, 133634.51it/s]\n",
      "10000000it [01:13, 135903.66it/s]\n",
      "10000000it [01:14, 134088.84it/s]\n",
      "10000000it [01:14, 134552.97it/s]\n",
      "10000000it [01:13, 135786.87it/s]\n",
      "10000000it [01:14, 134776.51it/s]\n",
      "7013682it [00:51, 135126.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    file = open('resource/token_context_split'+str(i),'r')\n",
    "    temp_tc = defaultdict(int)\n",
    "    temp_t = defaultdict(int)\n",
    "    temp_c = defaultdict(int)\n",
    "    \n",
    "    for line in tqdm(file):\n",
    "        t = line.split()[0]\n",
    "        context_words = line.split()[1:]\n",
    "        temp_t[t] += 1\n",
    "        for c in context_words:\n",
    "            temp_c[c] += 1\n",
    "            temp_tc[(t,c)] +=1\n",
    "    with open('resource/tc_'+str(i)+'.pkl', 'wb') as tc_file:\n",
    "        pickle.dump(temp_tc,tc_file)\n",
    "    \n",
    "    \n",
    "    with open('resource/t_'+str(i)+'.pkl','wb') as t_file:\n",
    "        pickle.dump(temp_t,t_file)\n",
    "        \n",
    "    with open('resource/c_'+str(i)+'.pkl','wb') as c_file:\n",
    "        pickle.dump(temp_c,c_file)\n",
    "    \n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [15:01<00:00, 75.14s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main_tc = defaultdict(int)\n",
    "main_t = defaultdict(int)\n",
    "main_c = defaultdict(int)\n",
    "\n",
    "for i in tqdm(range(12)):\n",
    "    with open('./resource/tc_{}.pkl'.format(str(i)),'rb') as tc:\n",
    "        temp_tc = pickle.load(tc)\n",
    "        for key, item in temp_tc.items():\n",
    "            main_tc[\",\".join(key)] += item\n",
    "          \n",
    "    with open('./resource/t_{}.pkl'.format(str(i)),'rb') as t:\n",
    "        temp_t = pickle.load(t)\n",
    "        for key,item in temp_t.items():\n",
    "            main_t[key] +=item  \n",
    "            \n",
    "    with open('./resource/c_{}.pkl'.format(str(i)),'rb') as c:\n",
    "        temp_c = pickle.load(c)\n",
    "        for key,item in temp_c.items():\n",
    "            main_c[key] +=item\n",
    "\n",
    "with open('resource/main_tc.json','w') as tc_file:\n",
    "    json.dump(main_tc, tc_file)\n",
    "\n",
    "with open('resource/main_t.json','w') as t_file:\n",
    "    json.dump(main_t, t_file)\n",
    "    \n",
    "with open('resource/main_c.json','w') as c_file:\n",
    "    json.dump(main_c, c_file)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_List = []\n",
    "with open('resource/main_tc.csv','w') as tc_csv:\n",
    "    writer = csv.writer(tc_csv)\n",
    "    for key,item in main_tc.items():\n",
    "        temp = key.split(',')\n",
    "        temp.append(item)\n",
    "        csv_List.append(temp)\n",
    "        \n",
    "    writer.writerows(csv_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語文脈行列の作成\n",
    "83の出力を利用し、単語文脈行列$X$を作成せよ。ただし、行列$X$の各要素$X_{tc}$は次のようにする。\n",
    "* $f(t,c) \\geq 10 \\Rightarrow X_{tc} = PPMI(t,c) = max\\{\\log{\\frac{N\\times f(t,c)}{f(t,*) \\times f(*, c)}}, 0\\}$\n",
    "* $f(t,c) < 10 \\Rightarrow X_{tc} = 0$\n",
    "\n",
    "ここで$PPMI(t,c)$はPositive Pointwise Mutual Infomation(正の相互情報量)と呼ばれる統計量である。なお、行列$X$の行数、列数は数百万オーダーになり、行列すべての要素を主記憶上に乗せることは無理なので注意すること。幸い、行列Xのほとんどの要素は0になるので、非0の要素だけ書き出す。\n",
    "\n",
    "```bash\n",
    "    wc -l ./resource/main_tc.csv\n",
    "```\n",
    "でNを調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b81a36f38505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./resource/main_tc.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtc_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmain_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "with open('./resource/main_t.json','r') as t_file:\n",
    "    dict_t = json.load(t_file)\n",
    "    dict_t_keys = list(dict_t.keys())\n",
    "with open('./resource/main_c.json','r') as c_file:\n",
    "    dict_c = json.load(c_file)\n",
    "    dict_c_keys = list(dict_c.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123075730it [58:47, 34891.63it/s]\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "cols = []\n",
    "data =[]\n",
    "N =  123075730\n",
    "with open('./resource/main_tc.csv','r') as tc_file:\n",
    "    reader = csv.reader(tc_file)\n",
    "    for i,row in tqdm(enumerate(reader)):\n",
    "        if int(row[2]) >= 10:\n",
    "            temp = np.log((N*int(row[2]))/(dict_t[row[0]]*dict_c[row[1]]))\n",
    "            if temp != 0:\n",
    "                rows.append(dict_t_keys.index(row[0]))\n",
    "                cols.append(dict_c_keys.index(row[1]))\n",
    "                data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resource/rows.pkl','wb') as row_file:\n",
    "    pickle.dump(rows,row_file)\n",
    "    \n",
    "with open('./resource/cols.pkl','wb') as col_file:\n",
    "    pickle.dump(cols,col_file)\n",
    "    \n",
    "with open('./resource/data.pkl','wb') as data_file:\n",
    "    pickle.dump(data,data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 主成分分析による次元圧縮\n",
    "84で得られた単語文脈行列に対して、主成分分析を適用し、単語の意味ベクトルを300次元に圧縮せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resource/rows.pkl','rb') as row_file:\n",
    "    rows = pickle.load(row_file)\n",
    "    \n",
    "with open('./resource/cols.pkl','rb') as col_file:\n",
    "    cols = pickle.load(col_file)\n",
    "    \n",
    "with open('./resource/data.pkl','rb') as data_file:\n",
    "    data = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_row = len(dict_t)\n",
    "length_col = len(dict_c)\n",
    "\n",
    "X = sparse.coo_matrix((data, (rows, cols)),shape=(length_row,length_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715039, 1739536)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "X300dim = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./resource/X300dim.npy',X300dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86.単語ベクトルの表示\n",
    "85で得た単語の意味ベクトルを読み込み、\"United States\"のベクトルを表示せよ。ただし、\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.27970484e+01,  5.77445233e+00,  4.40833334e+00,  1.02726818e+01,\n",
       "       -7.88510507e+00, -5.18350564e-01,  6.49889441e+00, -4.15398408e+00,\n",
       "       -7.68903868e+00,  1.13597390e-01,  2.03063086e+00, -5.21957474e+00,\n",
       "       -2.96056246e+00, -3.18377984e+00, -7.61399844e-01, -3.29820382e+00,\n",
       "       -1.58915078e+00,  1.01529873e-01,  5.35748852e-01, -7.30815457e-01,\n",
       "        2.33993160e-01,  1.19994821e+00,  4.86317120e+00,  5.58557639e-01,\n",
       "       -1.31066916e+00,  4.19081722e+00,  3.33579726e+00,  4.30344407e-01,\n",
       "       -3.56524589e+00, -1.76653084e+00,  9.26803288e-01, -5.46630096e+00,\n",
       "        1.39431944e+00,  8.32795348e-01, -3.97909784e+00,  6.18217288e+00,\n",
       "        3.08817651e-01,  7.97488301e-01, -2.25564231e-01,  9.45901213e-02,\n",
       "       -3.71322125e+00, -5.85453631e+00,  1.18087828e+00, -4.97556555e+00,\n",
       "        5.43871182e+00,  2.23812650e+00, -1.96665694e-01, -1.08507876e+00,\n",
       "       -8.17299060e-01,  3.61558335e+00, -8.54163162e-01, -4.62555251e+00,\n",
       "        2.25217188e-01,  1.28996917e+00,  1.08049034e+00, -2.90839976e+00,\n",
       "       -4.20490795e+00, -7.36726348e-01, -4.34602277e-01,  1.57283248e+00,\n",
       "       -2.69542854e+00, -7.44297688e-01, -2.40584518e+00,  9.41700178e-01,\n",
       "        1.81887153e+00, -1.09107422e+00,  1.08692731e+00,  5.12368211e-01,\n",
       "        3.70848816e+00, -2.41917951e+00, -9.31644616e-01,  2.31101247e+00,\n",
       "       -2.79097986e+00,  2.56546184e+00,  4.60400820e-01,  1.93769672e-03,\n",
       "       -1.51949068e+00,  3.44688127e+00, -3.98778706e-01,  5.12765808e-01,\n",
       "        2.11213689e-01, -1.30281694e+00,  8.10321335e-01,  5.43206973e-02,\n",
       "       -4.72028511e-03, -1.16675098e+00, -7.06571130e-02, -3.29026188e+00,\n",
       "       -9.29076611e-01, -3.28521676e-01, -7.71760015e-01,  8.58012466e-01,\n",
       "        6.60612194e-01, -1.00915593e+00, -2.59908035e-01, -1.62762418e-01,\n",
       "        9.51176683e-01, -8.31465468e-01, -6.98484890e-03, -2.46250077e+00,\n",
       "       -8.09677285e-01, -7.58189123e-01,  7.68409225e-02,  9.92256569e-01,\n",
       "       -1.53906297e+00, -1.49071690e+00,  8.27410833e-01,  1.53814853e-01,\n",
       "       -2.69764463e+00,  1.39301283e+00,  6.98694029e-01,  5.54301066e-01,\n",
       "        3.19916900e-01, -2.37952194e+00, -1.01475802e+00,  8.39697885e-01,\n",
       "       -5.58622182e-01,  3.66369385e-01,  1.18368283e+00, -1.12555726e+00,\n",
       "       -1.18103925e+00,  3.00873668e+00, -1.92277756e+00,  4.36743622e-02,\n",
       "        1.86170689e-01, -3.46427969e-01, -7.75546034e-01,  6.19725428e-01,\n",
       "        2.10174302e-01, -2.40653197e+00, -4.96616467e-01, -9.27677414e-01,\n",
       "        9.47097237e-01, -2.12720001e+00,  1.34740098e+00, -9.58321750e-01,\n",
       "       -8.60227993e-01,  2.50212088e-01,  2.45905737e+00,  3.77994492e-01,\n",
       "       -2.40557012e+00,  1.22503096e+00,  1.64293590e+00, -2.64882087e+00,\n",
       "       -3.30534371e-02,  1.45378613e-01,  1.39668655e+00,  1.97754218e+00,\n",
       "       -1.20459937e+00, -1.29466364e+00, -5.83602317e-01,  2.30021482e+00,\n",
       "        6.17178664e-01,  7.92656044e-01,  5.03187839e-02,  1.54299294e+00,\n",
       "       -1.55691165e+00, -6.05677518e-01,  1.30745390e-02, -1.65169438e+00,\n",
       "       -1.02749668e+00,  1.04250088e-01, -1.94492661e+00,  3.70954277e-02,\n",
       "       -1.82806182e+00,  1.22449428e+00, -1.77342299e+00,  4.13320415e-01,\n",
       "       -7.32450548e-01,  1.99732814e+00, -9.81776510e-01, -1.56083005e+00,\n",
       "       -1.49263806e-01,  9.57237208e-02, -1.48283781e+00, -2.47237381e+00,\n",
       "        8.70924847e-01,  1.14039193e-01,  6.13445981e-01,  1.38913345e+00,\n",
       "        1.07390079e+00,  1.51659813e+00, -3.71004229e-01, -1.74297399e-02,\n",
       "       -7.21737821e-01,  1.01965882e+00, -7.26561137e-02,  4.43580326e-01,\n",
       "       -7.09857797e-01, -1.20491980e+00,  1.96494929e+00, -5.79218721e-01,\n",
       "        3.99166753e+00,  7.30398997e-01, -1.03661840e+00, -6.35591576e-01,\n",
       "       -3.46550485e-01, -7.24971987e-01, -1.87365253e+00,  3.23374114e+00,\n",
       "       -1.21006889e-01,  1.31664826e+00,  1.49531474e+00, -7.03508036e-01,\n",
       "       -6.80217102e-01,  4.91485991e-02, -1.22193816e+00, -2.05346423e-01,\n",
       "        2.26277433e-01,  2.89146000e+00, -1.10551666e+00, -1.18456399e+00,\n",
       "       -8.46568222e-01,  5.63385478e-01,  2.12562579e-01,  7.91226041e-01,\n",
       "       -7.44719279e-02,  1.25395169e-01,  5.36613587e-01,  4.38895235e-01,\n",
       "        1.48609500e+00, -1.02789330e+00,  2.65899297e+00, -5.52979067e-01,\n",
       "       -1.36464595e+00,  7.32217811e-01,  1.83877815e-01,  3.22707976e+00,\n",
       "       -4.55189898e-01,  2.95765862e-01, -1.75026206e+00,  7.97123205e-01,\n",
       "        2.29233009e+00,  1.55269210e-01, -2.88912879e+00, -2.73205039e-01,\n",
       "       -1.23429417e+00, -2.13067874e+00, -2.85349304e-01,  1.66092053e-01,\n",
       "        1.85295195e-01,  8.67738363e-01,  3.72641085e-01,  1.93519241e+00,\n",
       "       -4.84750590e-01,  1.61378300e-02,  3.56434181e-01,  9.13339809e-01,\n",
       "        1.60994950e-01,  8.27777554e-01,  2.05146530e-01, -2.38215279e+00,\n",
       "        1.18571565e-01,  1.98733804e-01, -1.58488183e+00,  4.51931147e-01,\n",
       "        4.79672670e-01, -1.68087987e+00, -5.81347462e-02, -9.60356089e-01,\n",
       "        1.17197863e+00, -1.07107792e+00,  1.36538597e+00,  4.26950646e-01,\n",
       "       -1.20600639e+00, -1.27925298e+00, -1.05520473e+00,  2.06800088e+00,\n",
       "       -8.40628046e-01, -1.67471755e+00, -4.35735834e-01,  2.38768365e+00,\n",
       "       -9.94848810e-01, -8.11367162e-01, -1.19825339e+00, -3.37292370e-02,\n",
       "       -1.95817669e-01,  4.65839362e-01,  8.66709541e-01,  5.37671159e-01,\n",
       "        1.17548912e+00,  8.52540621e-01, -7.06401321e-02, -8.22613378e-02,\n",
       "       -2.07169768e-01,  2.24616117e+00,  1.33454237e+00, -1.31253083e-01,\n",
       "        4.67555947e-01, -5.47397708e-01, -1.07646355e+00, -4.93692722e-01,\n",
       "       -5.88426600e-01,  1.42718876e+00, -3.70927929e-01,  4.54529117e-01,\n",
       "        1.51008673e+00, -2.66906526e-01,  5.84504448e-01, -5.02974583e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UnitedState_index = dict_t_keys.index('United_States')\n",
    "#UnitedState_index = 273\n",
    "X300dim[UnitedState_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 単語の類似度\n",
    "85で得た単語ベクトルを読み込み、\"United States\"と\"U.S.\"のコサイン類似度を計算せよ。ただし、\"U.S.\"は内部的には\"U.S\"と表現されていることに注意せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7112571401815301"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_index = dict_t_keys.index(\"US\")\n",
    "#US_index = 1190\n",
    "def calc_cossim(vec1, vec2):\n",
    "    inner_product = np.dot(vec1,vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return inner_product/(norm1*norm2)\n",
    "\n",
    "US_vector = X300dim[US_index]\n",
    "United_States_vector = X300dim[UnitedState_index]\n",
    "\n",
    "cos_similarity = calc_cossim(US_vector,United_States_vector)\n",
    "cos_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88.類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み、\"England\"とコサイン類似度が高い10語と、その類似度を出力せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/.pyenv/versions/3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Scotland', 0.7605852369214897],\n",
       " ['Ireland', 0.6730711277946991],\n",
       " ['Wales', 0.6674960683642336],\n",
       " ['Australia', 0.6318441208524082],\n",
       " ['Britain', 0.593913842860392],\n",
       " ['France', 0.5666111035785446],\n",
       " ['Zealand', 0.5435728637656047],\n",
       " ['Yorkshire', 0.5377893857087538],\n",
       " ['London', 0.5254583203765075],\n",
       " ['Cornwall', 0.5201827203472809]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "England_vec = X300dim[dict_t_keys.index(\"England\")]\n",
    "\n",
    "result_List = []\n",
    "for i, key in enumerate(dict_t_keys):\n",
    "    if len(result_List) < 11:\n",
    "        result_List.append([key, calc_cossim(England_vec, X300dim[i])])\n",
    "        result_List = sorted(result_List,key=lambda x:x[1],reverse=True)\n",
    "    else:\n",
    "        temp_cossim = calc_cossim(England_vec, X300dim[i])\n",
    "        if result_List[-1][1] < temp_cossim:\n",
    "            result_List[-1] = [key, calc_cossim(England_vec, X300dim[i])]\n",
    "            result_List = sorted(result_List,key=lambda x:x[1], reverse=True)\n",
    "\n",
    "result_List[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89.加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み、vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し、そのベクトル類似度の高い10語とその類似度を出力せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/.pyenv/versions/3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['medallist', 0.2978921165189105],\n",
       " ['Muhlenberg', 0.27040080770255],\n",
       " ['Decatur', 0.2666470083779609],\n",
       " ['Paralympic', 0.2600490699019406],\n",
       " ['Fairfield', 0.24787179297623158],\n",
       " ['Nagano', 0.2379682882908366],\n",
       " ['Lamar', 0.23294836506941743],\n",
       " ['Macomb', 0.23289291965960707],\n",
       " ['Placer', 0.23241486725196886],\n",
       " ['Greenville', 0.22719739939274197],\n",
       " ['Riverside', 0.22507256052446664],\n",
       " ['Gwinnett', 0.2211096478067799],\n",
       " ['Diocesan', 0.22101289520528253],\n",
       " ['Helsinki', 0.22088225282195154],\n",
       " ['medallists', 0.21793807358108577],\n",
       " ['Goodwill', 0.21788502095898132],\n",
       " ['Boulder', 0.2145588101433165],\n",
       " ['Northeastern', 0.2130245907592803],\n",
       " ['Preziosa', 0.16575165265821334]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = X300dim[dict_t_keys.index(\"Spain\")] - X300dim[dict_t_keys.index(\"Madrid\")] + X300dim[dict_t_keys.index(\"Athens\")]\n",
    "result_List = []\n",
    "\n",
    "for i, key in enumerate(dict_t_keys):\n",
    "    if len(result_List) < 20:\n",
    "        result_List.append([key, calc_cossim(vector, X300dim[i])])\n",
    "        result_List = sorted(result_List,key=lambda x:x[1],reverse=True)\n",
    "    else:\n",
    "        temp_cossim = calc_cossim(England_vec, X300dim[i])\n",
    "        if result_List[-1][1] < temp_cossim:\n",
    "            result_List[-1] = [key, calc_cossim(vector, X300dim[i])]\n",
    "            result_List = sorted(result_List,key=lambda x:x[1], reverse=True)\n",
    "\n",
    "result_List[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
